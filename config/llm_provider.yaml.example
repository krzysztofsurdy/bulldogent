# LLM provider configuration
# Copy to llm_provider.yaml and keep only the providers you want to use.
# Only providers present in the real file will be registered.

openai:
  model_env: PROVIDER_OPENAI_MODEL
  temperature_env: PROVIDER_OPENAI_TEMPERATURE
  max_tokens_env: PROVIDER_OPENAI_MAX_TOKENS
  api_key_env: PROVIDER_OPENAI_API_KEY
  api_url_env: PROVIDER_OPENAI_API_URL          # optional — point at LiteLLM proxy, Ollama, etc.

bedrock:
  model_env: PROVIDER_BEDROCK_MODEL
  temperature_env: PROVIDER_BEDROCK_TEMPERATURE
  max_tokens_env: PROVIDER_BEDROCK_MAX_TOKENS
  region_env: PROVIDER_BEDROCK_REGION
  anthropic_version_env: PROVIDER_BEDROCK_ANTHROPIC_VERSION
  api_url_env: PROVIDER_BEDROCK_API_URL          # optional — e.g. LocalStack endpoint

vertex:
  model_env: PROVIDER_VERTEX_MODEL
  temperature_env: PROVIDER_VERTEX_TEMPERATURE
  max_tokens_env: PROVIDER_VERTEX_MAX_TOKENS
  project_id_env: PROVIDER_VERTEX_PROJECT_ID
  location_env: PROVIDER_VERTEX_LOCATION
  api_url_env: PROVIDER_VERTEX_API_URL           # optional — custom Vertex endpoint